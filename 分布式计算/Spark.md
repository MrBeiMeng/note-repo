# Spark ç¬”è®°

**Spark æ˜¯ä¸€ç§åŸºäºŽå†…å­˜çš„å¿«é€Ÿã€é€šç”¨ã€å¯æ‰©å±•çš„å¤§æ•°æ®åˆ†æžè®¡ç®—å¼•æ“Ž**

2009 - è‡³ä»Š



## å‰ç½®çŸ¥è¯†



### Hadoop HDFS çŸ¥è¯†è¡¥å……

æž¶æž„:

  ![image-20220917112209545](https://ccurj.oss-cn-beijing.aliyuncs.com/mac-typoraimage-20220917112209545.png)



**å¸¸ç”¨shell å‘½ä»¤**

> å‘½ä»¤è¡Œä¸­ `hadoop fs å…·ä½“å‘½ä»¤` ç­‰åŒäºŽ `hadoop dfs å…·ä½“å‘½ä»¤`

-cat æŸ¥çœ‹

-copyFromLocal -copyToLocal

-mv

-put

-rm

-rmdir







## äº†è§£Spark



### å¯¹æ¯”Hadoop

Hadoop æ˜¯ä¸€ä¸ªæ‰¹å¤„ç†å·¥å…·ï¼Œç”± java è¯­è¨€ç¼–å†™

Spark æ˜¯ä¸€ä¸ªæ‰¹æµä¸€ä½“çš„å¤„ç†å·¥å…·ï¼ŒåŸºäºŽå†…å­˜ï¼Œæ‰€ä»¥ä¼šæ¯”hadoopå¿«

![image-20220915214022209](https://ccurj.oss-cn-beijing.aliyuncs.com/mac-typoraimage-20220915214022209.png)

**ä¸€æ¬¡æ€§æ•°æ®è®¡ç®—ä¾‹å›¾**

> æ¡†æž¶åœ¨å¤„ç†æ•°æ®çš„æ—¶å€™ï¼Œä¼šä»Žå­˜å‚¨è®¾å¤‡ä¸­è¯»å–æ•°æ®ï¼Œè¿›è¡Œé€»è¾‘æ“ä½œï¼Œç„¶åŽå°†å¤„ç†çš„ç»“æžœé‡æ–°å­˜å‚¨åˆ°ä»‹è´¨ä¸­ã€‚ä½†æ˜¯å¦‚æ­¤æ“ä½œä¸€æ—¦é‡åˆ°æ•°æ®è¦é‡å¤ä¿®æ”¹ï¼Œå¤§é‡çš„IOæ“ä½œä¾¿ä¼šå½±å“æ•´ä½“çš„é€Ÿåº¦ã€‚Spark å…¶å®žå°±æ˜¯åœ¨é‡å¤æ“ä½œçš„è¿‡ç¨‹ä¸­é™¤åŽ»äº†ä¿å­˜åˆ°ç£ç›˜çš„é‚£ä¸€æ­¥ï¼Œç›´æŽ¥æ”¾åˆ°å†…å­˜è¿›è¡Œå¤„ç†ã€‚

![image-20220915200803410](https://ccurj.oss-cn-beijing.aliyuncs.com/mac-typoraimage-20220915200803410.png)



**ä¸èƒ½å®Œå…¨æ›¿ä»£ Hadoop**

- åœ¨è®¡ç®—å±‚é¢ï¼ŒSpark ç›¸æ¯”è¾ƒ Mr(MapReduce) æœ‰å·¨å¤§çš„æ€§èƒ½ä¼˜åŠ¿ï¼Œä½†è‡³ä»Šä»ç„¶æœ‰è®¸å¤šè®¡ç®—å·¥å…·åŸºäºŽ MR æ¡†æž¶ï¼Œæ¯”å¦‚éžå¸¸æˆç†Ÿçš„ Hiveã€‚
- Spark ä»…åšè®¡ç®—ï¼Œè€Œ Hadoop ç”Ÿæ€åœˆä¸ä»…æœ‰è®¡ç®—( Mr ) ä¹Ÿæœ‰å­˜å‚¨( HDFS ) å’Œ èµ„æºç®¡ç†è°ƒåº¦ (YARN) , HDFSå’ŒYARNä»æ˜¯è®¸å¤šå¤§å¤šæ•°ä½“ç³»çš„æ ¸å¿ƒæž¶æž„ã€‚





### æ ¸å¿ƒæ¨¡å—

![image-20220915201703053](https://ccurj.oss-cn-beijing.aliyuncs.com/mac-typoraimage-20220915201703053.png)

- **Spark Core** æä¾›æœ€åŸºç¡€æ ¸å¿ƒçš„åŠŸèƒ½ï¼Œå…¶ä»–æ¨¡å—éƒ½åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œæ‰©å±•
- **Spark SQL** æ“ä½œç»“æž„åŒ–æ•°æ®çš„æ¨¡å—
- **Spark Streaming** å¯¹å®žæ—¶æ•°æ®/æµå¼æ•°æ®è¿›è¡Œæ“ä½œçš„æ¨¡å—
- **Spark MLlib** æœºå™¨å­¦ä¹ æ“ä½œ
- **Spark Graphx** å›¾å½¢æŒ–æŽ˜è®¡ç®—



### **è¿è¡Œæ¨¡å¼**

- æœ¬åœ°æ¨¡å¼ (å•æœº) - **Local**  ä¸€èˆ¬ç”¨æ¥å¼€å‘å’Œæµ‹è¯• 

  > æœ¬åœ°æ¨¡å¼å°±æ˜¯ä»¥ä¸€ä¸ªç‹¬ç«‹çš„è¿›ç¨‹ï¼Œé€šè¿‡å…¶å†…éƒ¨çš„å¤šä¸ªçº¿ç¨‹æ¥æ¨¡æ‹Ÿæ•´ä¸ª Spark è¿è¡Œæ—¶çŽ¯å¢ƒ

- Standaloneæ¨¡å¼ (é›†ç¾¤) - **HDFS**

  > Spark ä¸­çš„å„ä¸ªè§’è‰²ä»¥ç‹¬ç«‹è¿›ç¨‹çš„å½¢å¼å­˜åœ¨ï¼Œå¹¶ç»„æˆSparké›†ç¾¤çŽ¯å¢ƒ

- Hadoop YARNæ¨¡å¼ (é›†ç¾¤)

  > Spark ä¸­çš„å„ä¸ªè§’è‰²è¿è¡Œåœ¨Kubernetesçš„å®¹å™¨å†…éƒ¨ï¼Œå¹¶ç»„æˆ Spark é›†ç¾¤çŽ¯å¢ƒ

  

  Kubernetes æ¨¡å¼ï¼Œäº‘æœåŠ¡æ¨¡å¼



### Spark ä¸Ž Yarn æž¶æž„è§’è‰²å¯¹æ¯”

Spark ä¸­ç”±4ç±»è§’è‰²ç»„æˆæ•´ä¸ªSpark çš„è¿è¡Œæ—¶çŽ¯å¢ƒ

- Master -- ã€ResourceManagerã€‘é›†ç¾¤ç®¡ç†è€…ï¼Œåˆ†é…æ•´ä¸ªé›†ç¾¤çš„èµ„æºã€‚
- Worker -- ã€NodeManagerã€‘å•ä¸ªæœºå™¨çš„ç®¡ç†è€…ï¼Œè´Ÿè´£åœ¨å•ä¸ªæœåŠ¡å™¨ä¸Šæä¾›è¿è¡Œå®¹å™¨ï¼Œç®¡ç†å½“å‰æœºå™¨çš„èµ„æºã€‚
- Driver  å•ä¸ª Sparkä»»åŠ¡çš„ç®¡ç†è€…ï¼Œç®¡ç† Executor çš„ä»»åŠ¡æ‰§è¡Œå’Œä»»åŠ¡åˆ†è§£åˆ†é…ï¼Œç±»ä¼¼ YARN çš„ApplicationMasterã€‚
- Executor å…·ä½“æ‰§è¡Œä»»åŠ¡çš„è¿›ç¨‹ï¼ŒSpark çš„å·¥ä½œä»»åŠ¡( Task )éƒ½ç”± Executor æ¥è´Ÿè´£æ‰§è¡Œã€‚



### å­¦ä¹ çŽ¯å¢ƒæ­å»º

ä¸‰å°Linux è™šæ‹ŸæœºæœåŠ¡å™¨ - æœ‰äº‘çš„ï¼Œä¸éœ€è¦æ­å»º

â€‹	node1: Master(HDFS\YARN\Spark) å’Œ Worker (HDFS\YARN\Spark)

â€‹	node2: Worker(HDFS\YARN\Spark)

â€‹	node3:Worker(HDFS\YARN\Spark) å’Œ Hive

 ç½‘æ®µ192.168.88.0

æ­å»ºçŽ¯å¢ƒæ•™ç¨‹ï¼šã€Spark pythonã€‘https://www.bilibili.com/video/BV1Jq4y1z7VP?p=13&vd_source=daf77da3a61c83fe6e450deea76785f6



### PySpark å’Œ Spark

PySpark æ˜¯ Spark çš„ä¸€ç§åŸºäºŽ python çš„æ“ä½œæ–¹å¼ï¼Œç¨‹åºè°ƒè¯•å¥½è¦ä¸Šä¼ åˆ° Spark åŽ»è¿è¡Œã€‚



### æœ¬æœºå¼€å‘çŽ¯å¢ƒæ­å»º

```python
pip install pyspark pyhive jieba
```

å¼€å‘å…¥å£

```python
conf = SparkConf().setAppName(appName).setMaster(master)
sc = SparkContext(conf= conf)
```





### Spark On Yarn æ¨¡å¼æ­å»º

**1 æœ¬è´¨**

- Master è§’è‰²ç”± YARN çš„ ResourceManage æ‹…ä»»ã€‚ã€ç®¡ç†ã€‘

- Worker è§’è‰²ç”± YARN çš„ NodeManageer æ‹…ä»»ã€‚ ã€ç®¡ç†ã€‘

- Driver è§’è‰²è¿è¡Œåœ¨ YARNå®¹å™¨ å†… æˆ– æäº¤ä»»åŠ¡çš„å®¢æˆ·ç«¯è¿›ç¨‹ã€‚ã€ä»»åŠ¡è®¡ç®—ã€‘

- çœŸæ­£å¹²æ´»çš„ Executor è¿è¡Œåœ¨ YARN æä¾›çš„å®¹å™¨å†…ã€‚ã€è¿è¡Œä»»åŠ¡ã€‘



**2 å‡†å¤‡**

1. éœ€è¦ Yarn é›†ç¾¤
2. éœ€è¦ Spark å®¢æˆ·ç«¯å·¥å…·ï¼Œæ¯”å¦‚ spark-submit, å¯ä»¥å°†Spark ç¨‹åºæäº¤åˆ° YARN ä¸­
3. éœ€è¦è¢«æäº¤çš„ä»£ç ç¨‹åºï¼Œ è¦è‡ªå·±å†™ã€‚



**3 çŽ¯å¢ƒæ­å»º**

æŒ‡è·¯ï¼šðŸ‘‰[ã€é»‘é©¬ç¨‹åºå‘˜Sparkå…¨å¥—è§†é¢‘æ•™ç¨‹ã€‘](https://www.bilibili.com/video/BV1Jq4y1z7VP?p=24&vd_source=daf77da3a61c83fe6e450deea76785f6)

 

**4 ä¸¤ç§éƒ¨ç½²æ¨¡å¼**

Cluster æ¨¡å¼(è¿è¡Œæ•ˆçŽ‡é«˜) ä¸Ž Clientæ¨¡å¼(å¯ä»¥æŸ¥çœ‹æ—¥å¿—)



### Spark ä»»åŠ¡æ“ä½œæµç¨‹

![image-20220917135752430](https://ccurj.oss-cn-beijing.aliyuncs.com/mac-typoraimage-20220917135752430.png)



### Spark å¹¿æ’­å˜é‡

> æ ‡è®°æˆ **å¹¿æ’­å˜é‡** å¯ä»¥é¿å…ä¸€ä¸ªå¯¹è±¡è¢«é‡å¤è°ƒç”¨ï¼Œå ç”¨ **ioèµ„æº**



### Spark ç´¯åŠ å™¨

> å› ä¸ºæ•°æ®æ˜¯åˆ†åŒºè¿›è¡Œè®¡ç®—çš„ï¼Œæ‰€ä»¥ç›´æŽ¥ count++ å¯èƒ½å¯¼è‡´åŠ ä¸åˆ°ã€‚7



### å†…æ ¸è°ƒåº¦



## RDD 

**â€”â€” å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†**

> åˆ†å¸ƒå¼è®¡ç®—éœ€è¦:åˆ†åŒºæŽ§åˆ¶ï¼Œ**Shuffle** æŽ§åˆ¶ï¼Œæ•°æ®å­˜å‚¨ï½œåºåˆ—åŒ–ï½œå‘é€ã€æ•°æ®è®¡ç®—APIã€ç­‰ç­‰ã€‚è¿™äº›åŠŸèƒ½å¾ˆéš¾æˆ–æ— æ³•é€šè¿‡ **Python** å†…ç½®çš„æœ¬åœ°é›†åˆå¯¹è±¡(List\å­—å…¸)ç­‰åŽ»å®Œæˆï¼Œåœ¨åˆ†å¸ƒå¼æ¡†æž¶ä¸­ï¼Œéœ€è¦ä¸€ä¸ªç»Ÿä¸€çš„æ•°æ®æŠ½è±¡å¯¹è±¡ï¼Œæ¥å®žçŽ°åˆ†å¸ƒå¼çš„åŠŸèƒ½ï¼Œè¿™ä¸ªå¯¹è±¡--å°±æ˜¯ **RDD**
>
> **RDD** æ˜¯ **Spark** ä¸­æœ€åŸºæœ¬çš„æ•°æ®æŠ½è±¡ï¼Œä»£è¡¨ä¸€ä¸ªä¸å¯å˜ã€å¯åˆ†åŒºï¼Œé‡Œé¢çš„å…ƒç´ å¯ä»¥å¹¶è¡Œè®¡ç®—çš„é›†åˆ
>
> - **Dataset**ï¼šä¸€ä¸ªæ•°æ®é›†åˆï¼Œç”¨äºŽå­˜æ”¾æ•°æ®
> - **Distributed**ï¼š**RDD** ä¸­çš„æ•°æ®æ˜¯ **åˆ†å¸ƒå¼å­˜å‚¨** çš„ï¼Œå¯ç”¨äºŽåˆ†å¸ƒå¼è®¡ç®—
> - **Resilient**ï¼š**RDD** ä¸­çš„æ•°æ®å¯ä»¥å­˜å‚¨åœ¨å†…å­˜æˆ–ç£ç›˜ä¸­
> - - Key-Value åž‹çš„ RDD å¯ä»¥æœ‰åˆ†åŒºå™¨
> - - RDD çš„åˆ†åŒºè§„åˆ’ï¼Œä¼šå°½é‡é è¿‘æ•°æ®æ‰€åœ¨çš„æœåŠ¡å™¨

![IMG_75C5ED0E79FC-1](https://ccurj.oss-cn-beijing.aliyuncs.com/mac-typoraIMG_75C5ED0E79FC-1.jpeg)



**SparkContextå¯¹è±¡**

> Spark RDD ç¼–ç¨‹çš„ç¨‹åºå…¥å£å¯¹è±¡ã€‚(ä¸è®ºä½•ç§è¯­è¨€)



**RDD** çš„ä¸¤ç§åˆ›å»ºæ–¹å¼

- é€šè¿‡å¹¶è¡ŒåŒ–é›†åˆåˆ›å»º å³ æœ¬åœ°å¯¹è±¡ è½¬ åˆ†å¸ƒå¼RDD  *parallelize*

  `rdd = spark_context.parallelize(data, part_num)`

- è¯»å–å¤–éƒ¨æ•°æ®æº å¦‚ è¯»å–æ–‡ä»¶

  `rdd2 = spark_context.textFile(data_path, part_num)` *--part_num ä¸æ˜¯100%ç”Ÿæ•ˆçš„*

  *è¯»å–æœ¬åœ°æ–‡ä»¶è¦åŠ ä¸Šï¼š`file://`*

  `rdd3 = spark_context.wholeTextFiles(dir_path,part_num)` *æ­¤apiåº”ç”¨äºŽå°æ–‡ä»¶ç‰¹åˆ«å¤šçš„åœºæ™¯*



**RDD ç®—å­**

> åˆ†å¸ƒå¼é›†åˆå¯¹è±¡ä¸Šçš„ **api** è¢«ç§°ä¸º **ç®—å­**
>
> **ç®—å­** åˆ†ä¸º **è½¬æ¢ç®—å­** å’Œ **åŠ¨ä½œç®—å­**
>
> - **Transformation ç®—å­**
>
>   è¿”å›žå€¼ä»ç„¶æ˜¯ **rdd** ï¼Œè¿™ç±»ç®—å­æ˜¯ **æ‡’åŠ è½½** çš„ï¼Œæ²¡æœ‰ **acion ç®—å­** å°†ä¸ä¼šå·¥ä½œ
>
> - **Action ç®—å­**
>
>   è¿”å›žå€¼ä¸æ˜¯ **rdd**



### **å¸¸ç”¨è½¬æ¢ç®—å­**

- **map** 

  > å°† **RDD** çš„æ•°æ® **åˆ†æ¡è¿›è¡Œå¤„ç†** ï¼Œå¤„ç†çš„é€»è¾‘ åŸºäºŽmapç®—å­ä¸­æŽ¥æ”¶çš„å¤„ç†å‡½æ•°ï¼Œè¿”å›žæ–°çš„ **RDD**

  è¯­æ³•ï¼š

  ```python
  rdd.map(func)
  # func : f:(T) -> U
  # example: rdd.map(lambda x: x + 2)
  ```

- flatMap

  > å¯¹ **RDD** æ‰§è¡Œ **mapæ“ä½œ** ï¼Œç„¶åŽè¿›è¡Œè§£é™¤åµŒå¥—æ“ä½œã€‚
  >
  > æœ‰æ—¶ **map** ä¸­ **è®¡ç®—çš„ç»“æžœå¯èƒ½ä¸ºä¸€ä¸ªlistå¯¹è±¡**ï¼Œç»“æžœå°±æ˜¯ **map** æ–¹æ³•è¿”å›žçš„æ•°æ®æ˜¯**åµŒå¥—äº†çš„**å¦‚ **[[1,2],[,3,4,5]]**ï¼ŒflatMap è´Ÿè´£**é™ä½Žä¸€ä¸ªç»´åº¦**ï¼Œç»“æžœä¸º ã€1ï¼Œ2ï¼Œ3ï¼Œ4ï¼Œ5ï¼Œ6ã€‘

  ```python
  rdd.flatMap(lambda x: [x + 1, x + 2]).collect()
  # [2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8]
  ```

- reduceByKey

  > é’ˆå¯¹ **KVåž‹** çš„ **RDD**ï¼Œè‡ªåŠ¨æŒ‰ç…§ **key** åˆ†ç»„ï¼Œç„¶åŽæ ¹æ®ä½ æä¾›çš„èšåˆé€»è¾‘ï¼Œå®Œæˆ  **ç»„å†…æ•°æ®(value)** çš„èšåˆæ“ä½œ

  ```python
  rdd,reduceByKey(func)
  # func : f:(V,V) -> V
  # rdd_key_value.reduceByKey(lambda value1, value2: value1 + value2).collect()
  #[('a', 1), ('a', 2), ('b', 2), ('c', 3), ('b', 5)] -> [('b', 7), ('c', 3), ('a', 3)]
  ```

- mapValues

  > é’ˆå¯¹ äºŒå…ƒå…ƒç»„RDD ï¼Œå¯¹å…¶å†…éƒ¨çš„ Value è¿›è¡Œ map æ“ä½œ

  è¯­æ³•ï¼š

  ```python
  rdd.mapValues(func)
  # func : f:(V) -> U
  ```

- groupBy

  > å°† **rdd** çš„æ•°æ®è¿›è¡Œåˆ†ç»„ï¼Œé€‚ç”¨äºŽ **å…ƒç»„æ•°æ®** 

  è¯­æ³•ï¼š

  ```python
  rdd.groupBy(func)
  # func: f(T) -> K
  ```

- filter

  > ä¿ç•™æƒ³è¦çš„æ•°æ®ï¼Œç»“æžœä¸º **TRUE** åˆ™ä¿ç•™

  è¯­æ³•ï¼š

  ```python
  rdd.filter(func)
  # func: (T) -> bool
  # rdd.filter(lambda x: x % 2 == 0).collect()
  ```

- distinct

  > å¯¹ RDD è¿›è¡ŒåŽ»é‡ï¼Œè¿”å›žæ–° RDD

  è¯­æ³•ï¼š

  ```python
  rdd.distinct(å‚æ•°1)
  # å‚æ•°1: åŽ»é‡åˆ†åŒºæ•°é‡ï¼Œä¸€èˆ¬ä¸ç”¨ä¼ 
  ```

- union

  > ä¸¤ä¸ª **RDD** åˆå¹¶æˆ1ä¸ª **RDD**ï¼Œç±»åž‹ä¸åŒä¹Ÿå¯ä»¥åˆå¹¶

  è¯­æ³•ï¼š

  ```python
  rdd1.union(rdd2)
  ```

- join

  > å¯¹ä¸¤ä¸ª **RDD** æ‰§è¡Œjoinæ“ä½œ(å¯ä»¥å®žçŽ°sql çš„å†…/å¤–è¿žæŽ¥) **åªèƒ½ç”¨äºŽäºŒå…ƒç»„å³KVRDD** ï¼Œå°±æ˜¯é€šè¿‡ **key** åŽ»join

  è¯­æ³•ï¼š

  ```python
  rdd1.join(rdd2)
  rdd1.leftOuterJoin(rdd2)
  rdd1.rightOuterJoin(rdd2)
  # [(1001,"zhangsan")]ï¼Œ[(1001,"é”€å”®éƒ¨")] -> [(1001,("å¼ ä¸‰","é”€å”®éƒ¨"))]
  ```

- intersection

  > æ±‚ä¸¤ä¸ª **RDD** çš„äº¤é›†ï¼Œè¿”å›žä¸€ä¸ªæ–° **RDD**

    è¯­æ³•ï¼š

    ```python
    rdd1.intersection(rdd2)
    ```

- glom

  > å°† **RDD** é€šè¿‡åˆ†åŒºè¿›è¡ŒåµŒå¥—

  è¯­æ³•:

  ```python
  rdd.glom()
  ```

- groupByKey

  > è‡ªåŠ¨æŒ‰ç…§ **key** è¿›è¡Œåˆ†ç»„

  è¯­æ³•ï¼š

  ```python
  rdd.groupByKey()
  # [('a',1),('a',2)] -> [('a',(1,1))]
  ```

- sortBy

  > å¯¹ RDD æ•°æ®è¿›è¡ŒæŽ’åºï¼ŒåŸºäºŽä½ æŒ‡å®šçš„æŽ’åºä¾æ®

  è¯­æ³•ï¼š

  ```python
  rdd.sortBy(func,ascending=False,numPartitioins=1)
  # func: (T) -> U å‘ŠçŸ¥ä½¿ç”¨ä»€ä¹ˆè¿›è¡ŒæŽ’åºï¼Œå¦‚ lambda x: x[1]
  ```

- sortByKey

  > é’ˆå¯¹ KVåž‹RDDï¼Œ æŒ‰ç…§key è¿›è¡ŒæŽ’åº

  è¯­æ³•ï¼š

  ```python
  sourtByKey(ascending=True,numPartitions=None,keyfunc=...)
  # ascending é»˜è®¤å‡åº
  # numPartitions æŒ‰ç…§å‡ ä¸ªåˆ†åŒºè¿›è¡ŒæŽ’åºå¦‚æžœå…¨å±€æœ‰åºï¼Œè®¾ç½®1
  # func: (K) -> U å¯ä»¥å¯¹keyè¿›è¡Œä¸€å®šçš„ä¿®é¥°æ“ä½œï¼Œæ–¹ä¾¿æŽ’åºï¼ŒåŒä¸Š
  ```



### å¸¸ç”¨æ‰§è¡Œç®—å­

- countByKey

  > ç»Ÿè®¡keyå‡ºçŽ°çš„æ¬¡æ•°(ä¸€èˆ¬é€‚ç”¨äºŽ KVåž‹ RDD)

  ```python
  rdd.countByKey()
  ```

- collect 

  > å°†å„ä¸ªåˆ†åŒºçš„æ•°æ®æ•´åˆèµ·æ¥ï¼Œç»Ÿä¸€åˆ° Driver ä¸­ï¼Œå½¢æˆä¸€ä¸ª List å¯¹è±¡

  ```python
  rdd.collect()
  ```

- flod

  > å’Œ reduce ä¸€æ ·ï¼ŒæŽ¥æ”¶ä¼ å…¥é€»è¾‘è¿›è¡Œèšåˆ

  ```python
  rdd.fold(10,lambda a, b: a + b)
  # range(1,10) ->  85
  ```

- first

  > å–å‡º RDD çš„ç¬¬ä¸€ä¸ªå…ƒç´ 

  ```python
  rdd.first()
  ```

- takeSample

  > éšæœºæŠ½æ · RDD çš„æ•°æ®

- takeOrdered

  > éšæœºå– RDDæŽ’åºåŽ çš„å‰Næ”¹ä¸ª

- foreach

  > å¯¹ RDD çš„æ¯ä¸€ä¸ªå…ƒç´ æ‰§è¡Œä½ æä¾›çš„æ“ä½œï¼Œä½†æ˜¯æ²¡æœ‰è¿”å›žå€¼

- saveAsTextFile

  > å°† RDD çš„æ•°æ®å†™å…¥æ–‡æœ¬æ–‡ä»¶ä¸­

  ```python
  rdd.saveAsTextFile("hdfs://node1:8020/output/11111")
  ```

- mapPartitions

  > ä¼ é€’ä¸€æ•´ä¸ªåˆ†åŒºçš„æ•°æ®

- foreachPartition

  > ä¸€æ¬¡å¤„ç†ä¸€æ•´ä¸ªåˆ†åŒº

- partitionBy

  > å¯¹ RDD è¿›è¡Œè‡ªå®šä¹‰åˆ†åŒºæ“ä½œ

- repartition

  > å¯¹ RDD è¿›è¡Œé‡æ–°åˆ†åŒº



### RDD æ˜¯è¿‡ç¨‹æ•°æ®

> RDD çš„æ•°æ®æ˜¯è¿‡ç¨‹æ•°æ®ï¼Œåªåœ¨å¤„ç†çš„è¿‡ç¨‹å­˜åœ¨ï¼Œä¸€æ—¦å¤„ç†å®Œæˆï¼Œå°±ä¸è§äº†ã€‚



### RDD çš„ç¼“å­˜

> Add.cache() # å¯ä»¥è®©

![IMG_0A2CF152530C-1](https://ccurj.oss-cn-beijing.aliyuncs.com/mac-typoraIMG_0A2CF152530C-1.jpeg)

### RDD çš„CheckPoint

> ç”¨äºŽå°† RDD çš„æ•°æ®ä¿å­˜èµ·æ¥ã€‚ **ä½†æ˜¯åªå…è®¸ç£ç›˜å­˜å‚¨**ã€‚ä¸Žç¼“å­˜ä¸åŒçš„æ˜¯ï¼ŒCheckPointå¹¶ä¸æ˜¯åˆ†æ•£å­˜å‚¨ï¼Œè€Œæ˜¯ç±»ä¼¼äºŽsaveAs.. å°†æ•°æ®èšåˆèµ·æ¥åˆ›å»ºåˆ°ä¸€ä¸ªåœ°æ–¹ã€‚

![IMG_268D38AE8284-1](https://ccurj.oss-cn-beijing.aliyuncs.com/mac-typoraIMG_268D38AE8284-1.jpeg)





## Spark æ¡ˆä¾‹ç»ƒä¹ 



### æœç´¢å¼•æ“Žæ—¥å¿—åˆ†æž

> ä½¿ç”¨æœç‹—å®žéªŒå®¤æä¾›çš„ç”¨æˆ·æŸ¥è¯¢æ—¥å¿—æ•°æ®ï¼Œä½¿ç”¨ Spark æ¡†æž¶ï¼Œè¿›è¡Œåˆ†æžã€‚

// todo





## Spark SQL

> SparkSQL æ˜¯éžå¸¸æˆç†Ÿçš„ æµ·é‡ç»“æž„åŒ–æ•°æ®å¤„ç†æ¡†æž¶
>
> - SparkSQL æœ¬èº«ååˆ†ä¼˜ç§€ï¼Œæ”¯æŒSQL è¯­è¨€\æ€§èƒ½å¼º\å¯ä»¥è‡ªåŠ¨ä¼˜åŒ–\APIç®€å•\å…¼å®¹HIVEç­‰
> - ä¼ä¸šå¤§é¢ç§¯åœ¨ä½¿ç”¨ SparkSQL å¤„ç†ä¸šåŠ¡æ•°æ®
>
>  ç‰¹ç‚¹ï¼š
>
> 1. **èžåˆ** å¯ä»¥å’Œpythonä»£ç æ— ç¼é›†æˆ
> 2. **ç»Ÿä¸€æ•°æ®è®¿é—®**ï¼ŒåŒä¸€å¥— è¯»å†™ä¸åŒçš„æ•°æ®æº
> 3. **Hive å…¼å®¹** å¯ä»¥ä½¿ç”¨ SparkSQL ç›´æŽ¥è®¡ç®—å¹¶ç”Ÿæˆ **Hive** æ•°æ®è¡¨
> 4. **æ ‡å‡†åŒ–é“¾æŽ¥** æ”¯æŒæ ‡å‡†åŒ– JDBC \ ODBC è¿žæŽ¥ï¼Œæ–¹ä¾¿å’Œå„ç§æ•°æ®åº“è¿›è¡Œæ•°æ®äº¤äº’ 



### æ•°æ®æŠ½è±¡

- Pandas - DataFrame				äºŒç»´è¡¨æ•°æ®ç»“æž„
- SparkCore - RDD                      æ— æ ‡å‡†æ•°æ®ç»“æž„ï¼Œå­˜å‚¨ä»€ä¹ˆæ•°æ®å‡å¯
- SparkSQL - DataFrameã€‚       äºŒç»´è¡¨æ•°æ®ç»“æž„



### DataFrame VS RDD

|                  | RDD              | DataFrame                  |
| ---------------- | ---------------- | -------------------------- |
| æœ‰åˆ†åŒºçš„         | æ˜¯               | æ˜¯                         |
| åˆ†å¸ƒå¼çš„         | æ˜¯               | æ˜¯                         |
| å¼¹æ€§çš„           | æ˜¯               | æ˜¯                         |
| å¯ä¿å­˜çš„æ•°æ®ç»“æž„ | **ä»»æ„æ•°æ®ç»“æž„** | **åªèƒ½å­˜å‚¨äºŒç»´è¡¨ç»“æž„æ•°æ®** |

> åœ¨ç»“æž„æ–¹é¢: **DataFrame**[**python**] ä¸­ **StructType** å¯¹è±¡æè¿°æ•´ä¸ª **DataFrame** çš„è¡¨ç»“æž„,**StructField** å¯¹è±¡æè¿°ä¸€ä¸ªåˆ—çš„ä¿¡æ¯
>
> ```python
>     struct_type = StructType(). \							# StructType
>         add("id", IntegerType(), False). \		# StructField
>         add("name", StringType(), True). \		# StructField
>         add("age", IntegerType(), False)			# StructField
> ```
>
> 



### sparkSession æ‰§è¡Œå¯¹è±¡

```python
spark_session = SparkSession.builder.appName("test").master("local[*]").getOrCreate()
# èŽ·å–context
spark_content = spark_session.sparkContext
```



### DataFrame å››ç§åˆ›å»ºæ–¹å¼

- é€šè¿‡ RDD è½¬æ¢ 1

  > ```python
  > # é€šè¿‡RDD è½¬æ¢
  > spark_context = spark_session.sparkContext
  > rdd = spark_context.textFile("file:///tmp/pycharm_project_93/data/stu_score.txt") \
  >     .map(lambda x: x.split(",")).map(lambda x: (x[0], int(x[1])))
  > # æž„å»º dataFrame
  > spark_session.createDataFrame(rdd, schema=['id', 'subject', 'score'])
  > ```

- é€šè¿‡ RDD è½¬æ¢ 2 **StructType**

  > ```python
  > schema = StructType() \
  >         .add("id", IntegerType(), nullable=False) \
  >         .add("name", StringType(), nullable=False) \
  >         .add("score", IntegerType(), nullable=False)
  >     
  > spark_session.createDataFrame(rdd,schema=schema)
  > ```

- é€šè¿‡ RDD è½¬æ¢3 **toDF**

  > ```python
  > data_frame2 =  rdd.toDF(["id", "subject", "score"]) # å¯ä»¥æŒ‡å®šschema
  > ```





## SparkSql test

```sql
DROP TABLE IF EXISTS data_source;
CREATE TABLE data_source
USING tablestore
OPTIONS(
endpoint="https://c00qovcof0zs.cn-shenzhen.ots-internal.aliyuncs.com",
access.key.id="LTAI5tBvXCnXJgD1JRMhAaFs",
access.key.secret="yaIOiAIR3Mj8g48uTga5t77QQ5wjAT",
instance.name="c00qovcof0zs",
table.name="daily_flow",
catalog='{"columns": {
        "date": {"type":"long"},
        "asin": {"type":"string"},
        "keyword": {"type":"string"},
        "ad_flow": {"type":"long"},
        "nature_flow": {"type":"long"},
        "ac_flow": {"type":"long"},
        "er_flow": {"type":"long"},
        "na_flow": {"type":"long"},
        "sb_flow": {"type":"long"},
        "sp_flow": {"type":"long"},
        "sd_flow": {"type":"long"}
    }}'
);    


DROP TABLE IF EXISTS data_source;
CREATE TABLE data_source
USING tablestore
OPTIONS(
endpoint="",
access.key.id="",
access.key.secret="",
instance.name="",
table.name="daily_flow",
catalog='{"columns": {
        "date": {"type":"long"},
        "asin": {"type":"string"},
        "keyword": {"type":"string"},
        "ad_flow": {"type":"long"}
        "nature_flow": {"type":"long"}
        "ac_flow": {"type":"long"}
        "er_flow": {"type":"long"}
        "na_flow": {"type":"long"}
        "sb_flow": {"type":"long"}
        "sp_flow": {"type":"long"}
        "sd_flow": {"type":"long"}
    }}'
);    


```

